{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=4\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "from utils.visualization import display_image_pair\n",
    "from utils.model import read_model_config, load_model\n",
    "from utils.dataset import read_dataset_config, load_dataset, Preprocessor\n",
    "from utils.correspondence import compute_pck_img, compute_pck_bbox, flip_points, rescale_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilled_model'\n",
    "dataset_config = '../dataset_config.yaml'\n",
    "model_config = '../eval_config.yaml'\n",
    "device_type = 'cuda'\n",
    "pck_threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model config\n",
    "model_config = read_model_config(model_config)[model_name]\n",
    "\n",
    "# Get model parameters\n",
    "image_size = model_config.get('image_size', (512, 512))\n",
    "grad_enabled = model_config.get('grad_enabled', False)\n",
    "rescale_data = model_config.get('rescale_data', False)\n",
    "image_range = model_config.get('image_range', (-1, 1))\n",
    "\n",
    "# Load model\n",
    "model = load_model(model_name, model_config)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(device_type)\n",
    "model.to(device)\n",
    "\n",
    "# Load dataset config\n",
    "dataset_config = read_dataset_config(dataset_config)\n",
    "\n",
    "# Define preprocessor\n",
    "preprocess = Preprocessor(image_size, image_range=image_range, rescale_data=rescale_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "dataset_name = list(dataset_config.keys())[0]\n",
    "print(f\"Evaluating dataset: {dataset_name}\")\n",
    "config = dataset_config[dataset_name]\n",
    "dataset = load_dataset(dataset_name, config)\n",
    "\n",
    "# Take first sample\n",
    "unprocessed_sample = dataset[0]\n",
    "\n",
    "# Use only 1 keypoint for Hedlin et al. because of time constraints\n",
    "if model_name == 'hedlin':\n",
    "    rand_i = torch.randint(0, unprocessed_sample['source_points'].shape[0], (1,)).item()\n",
    "    unprocessed_sample['source_points'] = unprocessed_sample['source_points'][rand_i, :].unsqueeze(0)\n",
    "    unprocessed_sample['target_points'] = unprocessed_sample['target_points'][rand_i, :].unsqueeze(0)\n",
    "\n",
    "# Visualize sample\n",
    "display_image_pair(unprocessed_sample, show_bbox=True)\n",
    "\n",
    "# Preprocess sample\n",
    "sample = preprocess(copy.deepcopy(unprocessed_sample))\n",
    "\n",
    "def batchify(sample):\n",
    "    batch = {}\n",
    "    for key in sample:\n",
    "        if key in ['source_image', 'target_image', 'source_bbox', 'target_bbox']:\n",
    "            batch[key] = sample[key].unsqueeze(0)\n",
    "        else:\n",
    "            batch[key] = [sample[key]]\n",
    "    return batch\n",
    "\n",
    "# Batchify sample\n",
    "sample = batchify(sample)\n",
    "\n",
    "# load images on device\n",
    "sample['source_image'] = sample['source_image'].to(device)\n",
    "sample['target_image'] = sample['target_image'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run through model\n",
    "with torch.set_grad_enabled(grad_enabled):\n",
    "    predicted_points = model.get_features_and_compute_correspondence(sample)[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sample = copy.deepcopy(unprocessed_sample)\n",
    "predicted_points = flip_points(predicted_points)\n",
    "if rescale_data:\n",
    "    predicted_points = rescale_points(predicted_points, image_size, predicted_sample['target_size'])\n",
    "predicted_sample['target_points'] = predicted_points\n",
    "\n",
    "display_image_pair(predicted_sample, show_bbox=False)\n",
    "display_image_pair(predicted_sample, show_bbox=False, ground_truth=unprocessed_sample['target_points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate PCK values\n",
    "target_points = unprocessed_sample['target_points']\n",
    "source_points = unprocessed_sample['source_points']\n",
    "target_bbox = unprocessed_sample['target_bbox']\n",
    "pck_img = compute_pck_img(predicted_points, target_points, image_size, pck_threshold)\n",
    "pck_bbox = compute_pck_bbox(predicted_points, target_points, target_bbox, pck_threshold)\n",
    "keypoints = len(source_points)\n",
    "\n",
    "print(f\"pck_img: {pck_img / keypoints}, pck_bbox: {pck_bbox / keypoints}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom and Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyfilechooser import FileChooser\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Create file chooser for source image\n",
    "source_fc = FileChooser()\n",
    "source_fc.title = '<b>Select Source Image</b>'\n",
    "source_fc.filter_pattern = ['*.jpg', '*.png', '*.jpeg']\n",
    "\n",
    "# Create file chooser for target image\n",
    "target_fc = FileChooser()\n",
    "target_fc.title = '<b>Select Target Image</b>'\n",
    "target_fc.filter_pattern = ['*.jpg', '*.png', '*.jpeg']\n",
    "\n",
    "# Layout file choosers side by side\n",
    "file_chooser_layout = widgets.HBox([source_fc, target_fc])\n",
    "display(file_chooser_layout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Load images\n",
    "source_image = Image.open(source_fc.selected)\n",
    "target_image = Image.open(target_fc.selected)\n",
    "\n",
    "# Get source and target features\n",
    "source_image_processed = preprocess.process_image(source_image)\n",
    "target_image_processed = preprocess.process_image(target_image)\n",
    "with torch.no_grad():\n",
    "    source_features = model.get_features(source_image_processed.unsqueeze(0).to(device), [''])\n",
    "    target_features = model.get_features(target_image_processed.unsqueeze(0).to(device), [''])\n",
    "\n",
    "if isinstance(source_features, list):\n",
    "    source_features = source_features[0]\n",
    "if isinstance(target_features, list):\n",
    "    target_features = target_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "class ColorPicker:\n",
    "    def __init__(self, colors, num_colors, shift):\n",
    "        cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors, N=num_colors)\n",
    "        self.gradient = [cmap(i / num_colors) for i in range(num_colors)]\n",
    "        self.shift = shift\n",
    "        self.num_colors = num_colors\n",
    "        self.current_index = 0\n",
    "\n",
    "    def __call__(self):\n",
    "        color = self.gradient[self.current_index]\n",
    "        self.current_index += 1\n",
    "        if self.current_index >= self.num_colors:\n",
    "            self.current_index = self.shift\n",
    "        return color\n",
    "        \n",
    "# Create a color picker\n",
    "colors = [\"#5FA1FF\", \"#FF00CC\", \"#FFFB00\"]\n",
    "color_picker = ColorPicker(colors, 10, 2)\n",
    "\n",
    "# Function to display images with optional equal height and circle drawing\n",
    "def display_images(source_image, target_image, equal_height=True, padding=20, circle_radius=20):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Set title\n",
    "    ax.set_title('Click on a point in the left image to run the model')\n",
    "   \n",
    "    # Calculate aspect ratios\n",
    "    source_aspect_ratio = source_image.size[0] / source_image.size[1]\n",
    "    target_aspect_ratio = target_image.size[0] / target_image.size[1]\n",
    "\n",
    "    if equal_height:\n",
    "        # Set the height of both images to be the same\n",
    "        new_height = source_image.size[1]\n",
    "\n",
    "        # Calculate new widths while maintaining the aspect ratios\n",
    "        source_new_width = int(new_height * source_aspect_ratio)\n",
    "        target_new_width = int(new_height * target_aspect_ratio)\n",
    "    else:\n",
    "        # Keep original dimensions if equal_height is False\n",
    "        new_height = source_image.size[1]\n",
    "        source_new_width = source_image.size[0]\n",
    "        target_new_width = target_image.size[0]\n",
    "\n",
    "    # Calculate the offset for the target image\n",
    "    offset = source_new_width + padding\n",
    "\n",
    "    # Draw the images\n",
    "    ax.imshow(source_image, extent=[0, source_new_width, 0, new_height])\n",
    "    ax.imshow(target_image, extent=[offset, offset + target_new_width, 0, new_height])\n",
    "\n",
    "    circles = []  # List to store circles\n",
    "\n",
    "    def onclick(event):\n",
    "        ix, iy = event.xdata, event.ydata\n",
    "\n",
    "        # Check if the click is within the bounds of the left image\n",
    "        if ix is not None and iy is not None and 0 <= ix < source_new_width and 0 <= iy < new_height:\n",
    "            color = color_picker()\n",
    "\n",
    "            # Create an underlying circle with 40% opacity\n",
    "            circle = Circle((ix, iy), radius=circle_radius, color=color, alpha=0.4)\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "            # Create a smaller overlying circle with 60% opacity\n",
    "            circle = Circle((ix, iy), radius=circle_radius * 0.6, color=color, alpha=0.6)\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "            circles.append(circle)\n",
    "            plt.draw()\n",
    "\n",
    "            # Scale the clicked point to the original image size and reverse y-axis\n",
    "            x = ix / source_new_width * source_image.size[0]\n",
    "            y = (1 - iy / new_height) * source_image.size[1]\n",
    "\n",
    "            # Run the model on the clicked point\n",
    "            batch = {\n",
    "                'source_features': source_features,\n",
    "                'target_features': target_features,\n",
    "                'source_points': [torch.tensor([[y, x]])],\n",
    "                'source_size': [[source_image.size[0], source_image.size[1]]],\n",
    "                'target_size': [[target_image.size[0], target_image.size[1]]]\n",
    "            }\n",
    "            predicted_points = model.compute_correspondence(batch)[0]\n",
    "            predicted_points = flip_points(predicted_points)\n",
    "            if rescale_data:\n",
    "                predicted_points = rescale_points(predicted_points, image_size, batch['target_size'])\n",
    "            predicted_point = predicted_points[0].cpu().detach().numpy()\n",
    "\n",
    "            # Scale the predicted point to the displayed image size and reverse y-axis\n",
    "            predicted_point[0] = predicted_point[0] / target_image.size[0] * target_new_width\n",
    "            predicted_point[1] = (1 - predicted_point[1] / target_image.size[1]) * new_height\n",
    "\n",
    "            circle = Circle((predicted_point[0] + offset, predicted_point[1]), radius=circle_radius,\n",
    "                            color=color, alpha=0.4)\n",
    "            ax.add_patch(circle)\n",
    "            circle = Circle((predicted_point[0] + offset, predicted_point[1]), radius=circle_radius * 0.6,\n",
    "                            color=color, alpha=0.6)\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "            # Draw a line between the clicked point and the predicted point\n",
    "            ax.plot([ix, predicted_point[0] + offset], [iy, predicted_point[1]], color=color,\n",
    "                    alpha=0.6)\n",
    "\n",
    "\n",
    "    # Connect the click event handler\n",
    "    fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "\n",
    "    # Set the axis limits to include both images\n",
    "    ax.set_xlim(0, offset + target_new_width)\n",
    "    ax.set_ylim(0, new_height)\n",
    "\n",
    "    # Disable the axis labels\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "display_images(source_image, target_image, equal_height=True, padding=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
